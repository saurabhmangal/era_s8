{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO-7t1Y7-hV4"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8kH16rnZ7wt_"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import sys, os\n",
    "\n",
    "import model\n",
    "from utils import plot_images\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy_exported\n"
     ]
    }
   ],
   "source": [
    "### this is for running in local ###\n",
    "try:\n",
    "    os.environ['HTTP_PROXY']='http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY']='http://185.46.212.90:80'\n",
    "    print (\"proxy_exported\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky3f_Odl-7um"
   },
   "source": [
    "## Data Transformations\n",
    "\n",
    "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YtssFUKb-jqx"
   },
   "outputs": [],
   "source": [
    "# # Train Phase transformations\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                      #transforms.RandomApply([transforms.CenterCrop(22), ], p=0.1),\n",
    "                                       #transforms.RandomApply([transforms.CenterCrop(22), ], p=0.1),\n",
    "                                       #transforms.Resize((28, 28)),\n",
    "                                       transforms.RandomAutocontrast(p=0.1),\n",
    "                                       transforms.RandomRotation((-7., 7.), fill=1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                    ])\n",
    "                            \n",
    "        \n",
    "# Test Phase transformations\n",
    "test_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                       ])\n",
    "\n",
    "\n",
    "# Test Phase transformations\n",
    "inv_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "                                                            std=[1/0.229, 1/0.224, 1/0.255]),\n",
    "                                       ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQciFYo2B1mO"
   },
   "source": [
    "# Dataset and Creating Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4A84rlfDA23",
    "outputId": "2cc0a4ce-205a-4bdf-ffb7-753398ed571f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomAutocontrast(p=0.1)\n",
      "               RandomRotation(degrees=[-7.0, 7.0], interpolation=nearest, expand=False, fill=1)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "train = datasets.CIFAR10('./data', train=True, download=True, transform=train_transforms)\n",
    "test = datasets.CIFAR10('./data', train=False, download=True, transform=test_transforms)\n",
    "print (train)\n",
    "print (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgldp_3-Dn0c"
   },
   "source": [
    "# Dataloader Arguments & Test/Train Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8OLDR79DrHG",
    "outputId": "66a9b95a-92e4-4194-c6a4-878b0ac5942a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available? True\n",
      "782\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "# CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"CUDA Available?\", cuda)\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# dataloader arguments - something you'll fetch these from cmdprmt\n",
    "dataloader_args = dict(shuffle=True, batch_size=64, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
    "\n",
    "# train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n",
    "print (len(train_loader))\n",
    "print (len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATASET\n"
     ]
    }
   ],
   "source": [
    "print (\"TRAIN DATASET\")\n",
    "batch_data_train, batch_label_train = next(iter(train_loader)) \n",
    "#figure_train = plot_images(batch_data_train, batch_label_train.tolist(), 12, 3, 'CIFAR10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATATEST\n"
     ]
    }
   ],
   "source": [
    "print ('TEST DATATEST')\n",
    "batch_data_test, batch_label_test = next(iter(test_loader)) \n",
    "# figure_test = plot_images(batch_data_test, batch_label_test.tolist(), 20, 4, 'CIFAR10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing\n",
    "\n",
    "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs.\n",
    "\n",
    "Let's write train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        # get samples\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "        # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
    "        # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.nll_loss(y_pred, target)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update pbar-tqdm\n",
    "\n",
    "        pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "\n",
    "        #pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "        train_acc.append(100*correct/processed)\n",
    "        \n",
    "    print (\" TRAIN ACCURACY: \",100*correct/processed, \"TRAIN LOSS: \", loss.item()),\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            \n",
    "            \n",
    "            for i in pred.tolist().values():\n",
    "                print (i)\n",
    "                #if (pred.eq(target.view_as(pred)) == False):\n",
    "                #    print (i)\n",
    "            \n",
    "            print (pred.eq(target.view_as(pred)))\n",
    "#             if (pred.eq(target.view_as(pred)) ==  False):\n",
    "#                 print ()\n",
    "            print (target)\n",
    "            print (pred.tolist())\n",
    "            print (type(pred))\n",
    "            transfom = pred.ToPILImage()\n",
    "            pred_new = inv_transforms(transfom)\n",
    "            \n",
    "            print (\"***********pred_new**********\")\n",
    "            print (pred_new)\n",
    "                      \n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    #print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "     #   test_loss, correct, len(test_loader.dataset),\n",
    "      #  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
    "    print (\" TEST ACCURACY: \",100*correct/len(test_loader.dataset), \"TEST LOSS: \", test_loss)\n",
    "    \n",
    "def plot_losses(train_losses, train_acc, test_losses, test_acc):\n",
    "    t = [t_items.item() for t_items in train_losses]\n",
    "    %matplotlib inline\n",
    "    \n",
    "    fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
    "    axs[0, 0].plot(t)\n",
    "    axs[0, 0].set_title(\"Training Loss\")\n",
    "    axs[1, 0].plot(train_acc[4000:])\n",
    "    axs[1, 0].set_title(\"Training Accuracy\")\n",
    "    axs[0, 1].plot(test_losses)\n",
    "    axs[0, 1].set_title(\"Test Loss\")\n",
    "    axs[1, 1].plot(test_acc)\n",
    "    axs[1, 1].set_title(\"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3-vp8X9LCWo"
   },
   "source": [
    "# Model Params for Batch Normalization\n",
    "Can't emphasize on how important viewing Model Summary is.\n",
    "Unfortunately, there is no in-built model visualizer, so we have to take external help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 48, 32, 32]           1,344\n",
      "              ReLU-2           [-1, 48, 32, 32]               0\n",
      "       BatchNorm2d-3           [-1, 48, 32, 32]              96\n",
      "         Dropout2d-4           [-1, 48, 32, 32]               0\n",
      "            Conv2d-5           [-1, 48, 32, 32]          20,784\n",
      "              ReLU-6           [-1, 48, 32, 32]               0\n",
      "       BatchNorm2d-7           [-1, 48, 32, 32]              96\n",
      "         Dropout2d-8           [-1, 48, 32, 32]               0\n",
      "            Conv2d-9           [-1, 32, 32, 32]           1,568\n",
      "        MaxPool2d-10           [-1, 32, 16, 16]               0\n",
      "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
      "             ReLU-12           [-1, 32, 16, 16]               0\n",
      "      BatchNorm2d-13           [-1, 32, 16, 16]              64\n",
      "        Dropout2d-14           [-1, 32, 16, 16]               0\n",
      "           Conv2d-15           [-1, 32, 16, 16]           9,248\n",
      "             ReLU-16           [-1, 32, 16, 16]               0\n",
      "      BatchNorm2d-17           [-1, 32, 16, 16]              64\n",
      "        Dropout2d-18           [-1, 32, 16, 16]               0\n",
      "           Conv2d-19           [-1, 32, 16, 16]           1,056\n",
      "        MaxPool2d-20             [-1, 32, 8, 8]               0\n",
      "           Conv2d-21             [-1, 16, 8, 8]           4,624\n",
      "             ReLU-22             [-1, 16, 8, 8]               0\n",
      "      BatchNorm2d-23             [-1, 16, 8, 8]              32\n",
      "        Dropout2d-24             [-1, 16, 8, 8]               0\n",
      "           Conv2d-25              [-1, 8, 8, 8]           1,160\n",
      "             ReLU-26              [-1, 8, 8, 8]               0\n",
      "      BatchNorm2d-27              [-1, 8, 8, 8]              16\n",
      "        Dropout2d-28              [-1, 8, 8, 8]               0\n",
      "           Conv2d-29             [-1, 10, 8, 8]             730\n",
      "             ReLU-30             [-1, 10, 8, 8]               0\n",
      "      BatchNorm2d-31             [-1, 10, 8, 8]              20\n",
      "        Dropout2d-32             [-1, 10, 8, 8]               0\n",
      "AdaptiveAvgPool2d-33             [-1, 10, 1, 1]               0\n",
      "           Conv2d-34             [-1, 10, 1, 1]             110\n",
      "================================================================\n",
      "Total params: 50,260\n",
      "Trainable params: 50,260\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.96\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 4.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "m = model.Net_batch_normalization().to(device)\n",
    "summary(m, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aE5Le6FYHhc8",
    "outputId": "a0178258-31c3-4644-b93f-a93181ad1760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 782/782 [00:04<00:00, 168.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TRAIN ACCURACY:  41.752 TRAIN LOSS:  1.5383192300796509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m train(m, device, train_loader, optimizer, epoch)\n\u001b[1;32m      8\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 60\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m     56\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# sum up batch loss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# get the index of the max log-probability\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m():\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m (i)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m#if (pred.eq(target.view_as(pred)) == False):\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m#    print (i)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(m.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1, verbose=False)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch),\n",
    "    train(m, device, train_loader, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    test(m, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drokW8wWODKq"
   },
   "source": [
    "# Let's Train and test our model\n",
    "\n",
    "This time let's add a scheduler for out LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "87RaqGSEOWDe",
    "outputId": "5d317b93-077b-4aa8-f340-439340a1bb27"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, train_acc, test_losses, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjO3RK9UEnvF"
   },
   "source": [
    "**********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Params for Group Normalization\n",
    "Can't emphasize on how important viewing Model Summary is.\n",
    "Unfortunately, there is no in-built model visualizer, so we have to take external help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.Net_group_norm().to(device)\n",
    "summary(m, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(m.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1, verbose=False)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    train(m, device, train_loader, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    test(m, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, train_acc, test_losses, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Params for Layer Normalization\n",
    "Can't emphasize on how important viewing Model Summary is.\n",
    "Unfortunately, there is no in-built model visualizer, so we have to take external help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.Net_layer_normalization().to(device)\n",
    "summary(m, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(m.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1, verbose=False)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    train(m, device, train_loader, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    test(m, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, train_acc, test_losses, test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
